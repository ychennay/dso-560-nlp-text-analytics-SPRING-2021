{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Dependencies-and-Load-in-Dataset\" data-toc-modified-id=\"Import-Dependencies-and-Load-in-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Dependencies and Load in Dataset</a></span></li><li><span><a href=\"#Pandas-Exploratory-Data-Analysis\" data-toc-modified-id=\"Pandas-Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pandas Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Show-the-First/Last-Rows-of-Dataset\" data-toc-modified-id=\"Show-the-First/Last-Rows-of-Dataset-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Show the First/Last Rows of Dataset</a></span></li><li><span><a href=\"#Drop-Unnecessary-Pandas-Column\" data-toc-modified-id=\"Drop-Unnecessary-Pandas-Column-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Drop Unnecessary Pandas Column</a></span></li><li><span><a href=\"#Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column\" data-toc-modified-id=\"Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Compute a New Pandas Series and Attach to Dataframe as Column</a></span></li><li><span><a href=\"#Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria\" data-toc-modified-id=\"Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>Count Number of Rows in Dataframe Meet Some Criteria</a></span></li><li><span><a href=\"#Filter-DataFrame-for-a-Subset-of-Rows\" data-toc-modified-id=\"Filter-DataFrame-for-a-Subset-of-Rows-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>Filter DataFrame for a Subset of Rows</a></span></li></ul></li><li><span><a href=\"#Difference-Between-extractall-and-findall\" data-toc-modified-id=\"Difference-Between-extractall-and-findall-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Difference Between <code>extractall</code> and <code>findall</code></a></span></li></ul></li><li><span><a href=\"#Regex-Character-Classes\" data-toc-modified-id=\"Regex-Character-Classes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regex Character Classes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Find-all-Tweets-That-Start-with-a-Number\" data-toc-modified-id=\"Find-all-Tweets-That-Start-with-a-Number-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Find all Tweets That Start with a Number</a></span></li><li><span><a href=\"#Find-all-@-Mentions\" data-toc-modified-id=\"Find-all-@-Mentions-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Find all @ Mentions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alternative-Method-to-Work-With-Lists-in-Pandas\" data-toc-modified-id=\"Alternative-Method-to-Work-With-Lists-in-Pandas-3.0.2.1\"><span class=\"toc-item-num\">3.0.2.1&nbsp;&nbsp;</span>Alternative Method to Work With Lists in Pandas</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Quantifiers\" data-toc-modified-id=\"Quantifiers-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Quantifiers</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Match-All-Phone-Numbers-(Link)\" data-toc-modified-id=\"Match-All-Phone-Numbers-(Link)-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Match All Phone Numbers (<a href=\"https://regexr.com/50v17\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Parse-Out-Zip-Codes-(Link)\" data-toc-modified-id=\"Parse-Out-Zip-Codes-(Link)-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Parse Out Zip Codes (<a href=\"https://regexr.com/50v1g\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Capture-Groups\" data-toc-modified-id=\"Capture-Groups-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Capture Groups</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)\" data-toc-modified-id=\"Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Example: Parsing out Weekday from Timestamp String (<a href=\"https://regexr.com/50v0l\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Example:-Parsing-Out-Domain-Names\" data-toc-modified-id=\"Example:-Parsing-Out-Domain-Names-5.0.2\"><span class=\"toc-item-num\">5.0.2&nbsp;&nbsp;</span>Example: Parsing Out Domain Names</a></span></li></ul></li><li><span><a href=\"#Non-Capture-Groups\" data-toc-modified-id=\"Non-Capture-Groups-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Non-Capture Groups</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Link)\" data-toc-modified-id=\"Example-1-(Link)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Example 1 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-2-(Link)\" data-toc-modified-id=\"Example-2-(Link)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Example 2 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-3-(Link)\" data-toc-modified-id=\"Example-3-(Link)-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Example 3 (<a href=\"https://regexr.com/50ush\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Named-Capture-Groups\" data-toc-modified-id=\"Named-Capture-Groups-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Named Capture Groups</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Using-Named-Capture-Groups-in-Pandas-for-Feature-Engineering\" data-toc-modified-id=\"Using-Named-Capture-Groups-in-Pandas-for-Feature-Engineering-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Using Named Capture Groups in Pandas for Feature Engineering</a></span></li></ul></li></ul></li><li><span><a href=\"#Finding-Repeat-Words-in-Reviews-Using-Backreferences\" data-toc-modified-id=\"Finding-Repeat-Words-in-Reviews-Using-Backreferences-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Finding Repeat Words in Reviews Using Backreferences</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Using-Named-Backreferences\" data-toc-modified-id=\"Using-Named-Backreferences-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>Using Named Backreferences</a></span></li><li><span><a href=\"#Using-Negative-Lookaheads\" data-toc-modified-id=\"Using-Negative-Lookaheads-7.0.2\"><span class=\"toc-item-num\">7.0.2&nbsp;&nbsp;</span>Using Negative Lookaheads</a></span></li><li><span><a href=\"#Negative-Lookbehind\" data-toc-modified-id=\"Negative-Lookbehind-7.0.3\"><span class=\"toc-item-num\">7.0.3&nbsp;&nbsp;</span>Negative Lookbehind</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-all-tweets-that-do-not-begin-with-a-hashtag-or-a-mention.\" data-toc-modified-id=\"Find-all-tweets-that-do-not-begin-with-a-hashtag-or-a-mention.-7.0.3.1\"><span class=\"toc-item-num\">7.0.3.1&nbsp;&nbsp;</span>Find all tweets that do not begin with a hashtag or a mention.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Exercises</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Use-named-capture-groups-to-find-the-subject-headings-for-these-emails.\" data-toc-modified-id=\"Use-named-capture-groups-to-find-the-subject-headings-for-these-emails.-8.0.0.1\"><span class=\"toc-item-num\">8.0.0.1&nbsp;&nbsp;</span>Use named capture groups to find the subject headings for these emails.</a></span></li><li><span><a href=\"#Find-all-hashtags-or-references-mentioned-in-the-tweets_df-dataset.-Store-them-as-two-separate-columns,-one-for-whether-it-is-either-a-@-or-#,-and-the-other-for-the-actual-content-(ie.-the-hello-part-of-#hello).\" data-toc-modified-id=\"Find-all-hashtags-or-references-mentioned-in-the-tweets_df-dataset.-Store-them-as-two-separate-columns,-one-for-whether-it-is-either-a-@-or-#,-and-the-other-for-the-actual-content-(ie.-the-hello-part-of-#hello).-8.0.0.2\"><span class=\"toc-item-num\">8.0.0.2&nbsp;&nbsp;</span>Find all hashtags or references mentioned in the <code>tweets_df</code> dataset. Store them as two separate columns, one for whether it is either a <code>@</code> or <code>#</code>, and the other for the actual content (ie. the <code>hello</code> part of <code>#hello</code>).</a></span></li><li><span><a href=\"#Use-named-capture-groups-to-provide-a-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.-The-output-should-be-a-list-of-dictionaries:\" data-toc-modified-id=\"Use-named-capture-groups-to-provide-a-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.-The-output-should-be-a-list-of-dictionaries:-8.0.0.3\"><span class=\"toc-item-num\">8.0.0.3&nbsp;&nbsp;</span>Use named capture groups to provide a <strong>list of email addresses</strong> for your security administrator to blacklist from your company's email servers. The output should be a list of dictionaries:</a></span></li><li><span><a href=\"#Use-a-negative-lookahead-to-capture-all-emails-except-for-the-ones-from-yahoo.com.\" data-toc-modified-id=\"Use-a-negative-lookahead-to-capture-all-emails-except-for-the-ones-from-yahoo.com.-8.0.0.4\"><span class=\"toc-item-num\">8.0.0.4&nbsp;&nbsp;</span>Use a negative lookahead to capture all emails except for the ones from <code>yahoo.com</code>.</a></span></li><li><span><a href=\"#Identify-any-IP-addresses-that-should-be-blacklisted\" data-toc-modified-id=\"Identify-any-IP-addresses-that-should-be-blacklisted-8.0.0.5\"><span class=\"toc-item-num\">8.0.0.5&nbsp;&nbsp;</span>Identify any IP addresses that should be blacklisted</a></span></li><li><span><a href=\"#The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.\" data-toc-modified-id=\"The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.-8.0.0.6\"><span class=\"toc-item-num\">8.0.0.6&nbsp;&nbsp;</span>The word \"AT&amp;T\" is not spelled correctly in the <code>tweets_df</code> dataset. Correct the misspelling.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies and Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_df: pd.DataFrame = pd.read_csv(\"tweets_pandas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Show the First/Last Rows of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Drop Unnecessary Pandas Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# get rid of some columns we don't care about\n",
    "tweets_df.drop(columns=[\"index\"], inplace=True)\n",
    "# preview the dataset\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Compute a New Pandas Series and Attach to Dataframe as Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# get length of tweets in characters\n",
    "tweets_df[\"tweet_length\"] = tweets_df[\"tweet\"].str.len()\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Count Number of Rows in Dataframe Meet Some Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# count number of times Obama appears in tweets\n",
    "tweets_df[\"tweet\"].str.contains(r'\\bObama\\b').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Filter DataFrame for a Subset of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# filter dataframe for only tweets that contain \n",
    "tweets_df[tweets_df[\"tweet\"].str.contains(r'\\bObama\\b')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between `extractall` and `findall`\n",
    "\n",
    "The `extractall` method will return a fanned-out multi-dimensional index. For example, in the example below, the primary index is `0`, but there are sub-indices for `stellargirl` (`0`), `loooooooovvvvvveee` (`1`), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df[\"tweet\"].str.extractall(r'\\b(\\w{3,})\\b').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `findall` method will return a list of results (or an empty list if there is no match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df[\"tweet\"].str.findall(r'\\b(\\w{3,})\\b').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Character Classes\n",
    "\n",
    "There are often shortcut keywords you can use instead of typing out every possible character you want to match against. For instance, instead of `[a-zA-Z0-9]`, for all practical purposes, you can type out `/w`.\n",
    "\n",
    "![Character Classes](images/character_chars.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Find all Tweets That Start with a Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "* [Non-Match Example](https://regexr.com/50ur7)\n",
    "* [Match Example](https://regexr.com/50urj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df[\"tweet\"].str.contains(r'^[0-9]')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Find all @ Mentions\n",
    "We'll use the `\\w` character class to match for mentions (ie. `@ychennay`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "tweets_df[\"mentions\"] = tweets_df[\"tweet\"].str.findall(r'@\\w+')\n",
    "tweets_df[tweets_df[\"mentions\"].apply(len).gt(0)] # gt is the same as >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "#### Alternative Method to Work With Lists in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# Convert the mentions column to boolean. Because an empty list is False, and a non-empty list is True, this filters\n",
    "# out all non-empty lists.\n",
    "tweets_df[tweets_df[\"mentions\"].astype(bool)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifiers\n",
    "\n",
    "Quantifiers let you specify how many times a character or group should be matched.\n",
    "![Quantifiers](images/quantifiers.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Match All Phone Numbers ([Link](https://regexr.com/50v17))\n",
    "In the unwanted callers dataset (`unwanted_calls.csv`), parse out all phone numbers.\n",
    "\n",
    "We'll only consider for the time being phone numbers that follow the format `123-456-7890`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "unwanted_calls_df: pd.DataFrame = pd.read_csv(\"unwanted_calls.csv\")\n",
    "unwanted_calls_df[\"caller_id_number\"].str.findall(r'\\d{3}-\\d{3}-\\d{4}').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Parse Out Zip Codes ([Link](https://regexr.com/50v1g))\n",
    "In the `location_center_point_of_the_zip_code` field, we store both zip codes as well as geolocation data (latitudes and longitudes). We'll only consider zip codes with 5 digits (not the +4 digit delivery route)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "unwanted_calls_df[\"location_center_point_of_the_zip_code\"].str.findall(r'\\s\\d{5}\\n').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture Groups\n",
    "\n",
    "[Oracle documentation on Capture Groups:](https://docs.oracle.com/javase/tutorial/essential/regex/groups.html)\n",
    "> Capturing groups are a way to treat **multiple characters as a single unit**. They are created by placing the characters to be grouped inside a set of parentheses. For example, the regular expression `(dog)` creates a single group containing the letters `d`, `o`, and `g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Example: Parsing out Weekday from Timestamp String ([Link](https://regexr.com/50v0l))\n",
    "\n",
    "In the `date` field of `tweets_df`, we have timestamp strings that look like this: `Mon May 11 03:22:30 UTC 2009`. We want to parse out the weekday from this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "tweets_df[\"day_of_week\"] = tweets_df[\"date\"].str.extract(r'^(\\w{3})\\b')\n",
    "tweets_df.sample(len(tweets_df)).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Example: Parsing Out Domain Names\n",
    "We want to capture the domain names of different websites. Here, we need to escape the `.` part of `www.google.com`. In regex, `.` means \"anything\". To actually indicate we want to match for the literal `.` period character, we need to escape it, using `\\.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "websites = open(\"list_of_websites.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "cleaned_websites = list(map(lambda website: re.sub(r'\\n', '', website), websites))\n",
    "regex = r'https?:\\/\\/www\\.(\\w+)\\.\\w+'\n",
    "domain_names = pd.Series(cleaned_websites).str.extract(regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Capture Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to group multiple characters into a single unit to apply regex operations on them, but you don't\n",
    "want to actually capture or return their result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Example 1 [(Link)](https://regexr.com/50t7c)\n",
    "Using non-capture groups to match for optional text:\n",
    "\n",
    "You want to capture both `child` and `children` in your text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"The word child is singular, but the word children is plural.\"\n",
    "re.findall(r'\\bchild(?:ren)?\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Example 2 [(Link)](https://regexr.com/50t7c)\n",
    "Find all the mentions in the tweets. Mentions start with `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "tweets_df[\"tweet\"].str.findall(r'(@\\w+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Example 3 ([Link](https://regexr.com/50ush))\n",
    "Here we want to match for the dollar amount, but we don't want to include the currency notation (`$` or `USD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "regex_pattern = \"(?:\\$|(?:USD))([0-9]+)\\.([0-9][0-9])\"\n",
    "text = \"Average fast food wage is $9.08, but inflation has increased USD9.23\"\n",
    "results = re.findall(regex_pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Capture Groups\n",
    "\n",
    "Often, we might have a hard time keeping track of all the capture groups in our regex. We can use **named capture groups** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "file = open(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")\n",
    "reviews = file.read()\n",
    "\n",
    "regex_pattern = r'1?\\d:[0-5]\\d\\s?(am|pm)'\n",
    "\n",
    "re.findall(regex_pattern, reviews)[:5] # what's wrong with this expression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex_pattern = r'1?\\d:[0-5]\\d\\s?(?:am|pm)'\n",
    "re.findall(regex_pattern, reviews)[:5] # this looks much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now, we want to capture the hour, minute, and time of day indicator\n",
    "regex_pattern = r'(?P<hour>1?\\d):(?P<minute>[0-5]\\d)\\s?(?P<indicator>:am|pm)'\n",
    "file.seek(0) # reset the index position\n",
    "reviews = file.readlines()\n",
    "for review in reviews:\n",
    "    match = re.search(regex_pattern, review) # this looks much better\n",
    "    if match:\n",
    "        print(match.groupdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Named Capture Groups in Pandas for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reviews_df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_df[\"review\"].str.extract(r'(?P<hour>1?\\d):(?P<minute>[0-5]\\d)\\s?(?P<indicator>:am|pm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Repeat Words in Reviews Using Backreferences\n",
    "You can refer to capture groups earlier in your expression using **backreferences**. For instance, `\\1` means the first capture group in your expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regex_pattern = r'(\\b\\w*burgers?\\b).+\\b\\1\\b'\n",
    "file.seek(0)\n",
    "reviews = file.readlines()\n",
    "\n",
    "for review in reviews:\n",
    "    match = re.search(regex_pattern, review)\n",
    "    if match:\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Named Backreferences\n",
    "\n",
    "If you named a group `BURGER`, you can reference it later with `(?P=BURGER)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regex_pattern = r'(?P<BURGER>\\b\\w*burgers?\\b).+\\b(?P=BURGER)\\b'\n",
    "file.seek(0)\n",
    "reviews = file.readlines()\n",
    "\n",
    "for review in reviews:\n",
    "    match = re.search(regex_pattern, review)\n",
    "    if match:\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Negative Lookaheads\n",
    "\n",
    "Whenever you are thinking *I need to match this expression, but only when the piece of text doesn't say X*, you can use **negative lookaheads**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to match only the timestamps in McDonalds reviews that happen before **12pm**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = r'(1?\\d):([0-5]\\d)\\s?(?!pm)'\n",
    "file.seek(0)\n",
    "reviews = file.readlines()\n",
    "for review in reviews[:100]:\n",
    "    match = re.search(regex_pattern, review)\n",
    "    if match:\n",
    "        print(match.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we still want to capture the time of day indicator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = r'(1?\\d):([0-5]\\d)\\s?(?!pm)(\\w{2})'\n",
    "file.seek(0)\n",
    "reviews = file.readlines()\n",
    "for review in reviews[:100]:\n",
    "    match = re.search(regex_pattern, review)\n",
    "    if match:\n",
    "        print(match.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Lookbehind\n",
    "#### Find all tweets that do not begin with a hashtag or a mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = r'^(?<!@)([\\w\\s\\.:]+)'\n",
    "tweets_df[\"extracted\"] = tweets_df[\"tweet\"].str.extract(regex_pattern)\n",
    "tweets_df[[\"tweet\", \"extracted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "You'll be using the the `tweets_df` Pandas dataframe and `fraudulent_emails.txt` text files for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "email_text = open(\"fraudulent_emails.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Use named capture groups to find the subject headings for these emails.\n",
    "**Hint**: Look for the subject line within the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Find the subject headings for these emails.\n",
    "email_text = open(\"fraudulent_emails.txt\").read()\n",
    "re.findall(r'Subject:\\s(.+)', email_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "#### Find all hashtags or references mentioned in the `tweets_df` dataset. Store them as two separate columns, one for whether it is either a `@` or `#`, and the other for the actual content (ie. the `hello` part of `#hello`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# Find all hashtags mentioned in the tweets dataset. Store it as a separate column called hashtags.\n",
    "# The \\B is \"not a word boundary\", and it matches because the # (and the @ in the previous example) is not \n",
    "# considered part of a word. Therefore, you need to opposite of a word boundary (a non-word boundary) to match\n",
    "# the case where it begins with a non-word character.\n",
    "\n",
    "# This will correctly NOT match text like she#he, sometext#someothertext\n",
    "tweets_df[\"tweet\"].str.findall(r'\\B(#\\w+)\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Use named capture groups to provide a **list of email addresses** for your security administrator to blacklist from your company's email servers. The output should be a list of dictionaries:\n",
    "\n",
    "```python\n",
    "[\n",
    " {\n",
    "     \"username\": \"yuchen\",\n",
    "     \"domain\": \"gmail.com\n",
    " },\n",
    "    # ...\n",
    "]\n",
    "```\n",
    "\n",
    "* Not all emails are malicious! Provide only the list of email addresses from where the email originates from. **Hint**: identify the pattern in the emails that tells you the source of the email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "regex_pattern = r'Return-Path: \\<(\\w+@\\w+\\.\\w{2,6})\\>'\n",
    "set(re.findall(regex_pattern, email_text)) # this matches only basic emails (myemail@gmail.com)\n",
    "\n",
    "regex_pattern = r'Return-Path: \\<(\\w+@\\w+\\.[\\w+\\.]{1,})\\>'\n",
    "set(re.findall(regex_pattern,email_text)) # this matches more complicated emails with multiple domain names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a negative lookahead to capture all emails except for the ones from `yahoo.com`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Identify any IP addresses that should be blacklisted\n",
    "\n",
    "An IPv4 address goes from **1.1.1.1 to 255.255.255.255**. For now, just worry about the number of digits, not whether the value in the address is above `255`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r'[1-2]?[0-9]{1,2}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}', email_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### The word \"AT&T\" is not spelled correctly in the `tweets_df` dataset. Correct the misspelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[\"tweet\"].str.replace(r'AT&amp;', 'AT&')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
